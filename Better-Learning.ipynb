{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Learning curves are plots that show changes in learning performance over time in terms of experience.\n",
    "* Learning curves of model performance on the train and validation datasets can be used to diagnose an underfit, overfit, or well-fit model.\n",
    "* Learning curves of model performance can be used to diagnose whether the train or validation datasets are not relatively representative of the problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimization Learning Curves: Learning curves calculated on the metric by which the parameters of the model are being optimized, e.g. loss.\n",
    "* Performance Learning Curves: Learning curves calculated on the metric by which the model will be evaluated and selected, e.g. accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfit Learning Curves\n",
    "* Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set.\n",
    "* An underfit model can be identified from the learning curve of the training loss only. It may show a flat line or noisy values of relatively high loss, indicating that the model was unable to learn the training dataset at all.\n",
    "* A plot of learning curves shows underfitting if:\n",
    "* The training loss remains flat regardless of training.\n",
    "* The training loss continues to decrease until the end of training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss Examples\n",
    "\n",
    "![Training Loss 1](images/training-loss.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss Example\n",
    "![Training Loss 2](images/training-loss-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit Learning Curves\n",
    "\n",
    "* Overfitting refers to a model that has learned the training dataset too well, including the statistical noise or random fluctuations in the training dataset.\n",
    "* fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to a phenomenon known as overfitting the data, which essentially means they follow the errors, or noise, too closely.\n",
    "* The problem with overfitting, is that the more specialized the model becomes to training data, the less well it is able to generalize to new data, resulting in an increase in generalization error. This increase in generalization error can be measured by the performance of the model on the validation dataset.\n",
    "\n",
    "* This often occurs if the model has more capacity than is required for the problem, and, in turn, too much flexibility. It can also occur if the model is trained for too long. A plot of learning curves shows overfitting if:\n",
    "* The plot of training loss continues to decrease with experience.\n",
    "* The plot of validation loss decreases to a point and begins increasing again.\n",
    "\n",
    "* The inflection point in validation loss may be the point at which training could be halted as experience after that point shows the dynamics of overfitting. The example plot below demonstrates a case of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Curves\n",
    "\n",
    "![Training Loss 1](images/overfitt-img.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Fit Learning Curves\n",
    "\n",
    "### A good fit is the goal of the learning algorithm and exists between an overfit and underfit model. A good fit is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values.\n",
    "\n",
    "* The plot of training loss decreases to a point of stability.\n",
    "* The plot of validation loss decreases to a point of stability and has a small gap with the\n",
    "training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Fitting Curves\n",
    "\n",
    "![Training Loss 1](images/Good-fit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing Unrepresentative Datasets\n",
    "\n",
    "* An unrepresentative dataset means a dataset that may not capture the statistical characteristics relative to another dataset drawn from the same domain, such as between a train and a validation dataset. \n",
    "* This can commonly occur if the number of samples in a dataset is too small, relative to another dataset.\n",
    "\n",
    "### There are two common cases that could be observed; they are:\n",
    "\n",
    "* Training dataset is relatively unrepresentative.\n",
    "* Validation dataset is relatively unrepresentative.\n",
    "\n",
    "### An unrepresentative training dataset means that the training dataset does not provide sufficient information to learn the problem, relative to the validation dataset used to evaluate it. This may occur if the training dataset has too few examples as compared to the validation dataset. This situation can be identified by a learning curve for training loss that shows improvement and similarly a learning curve for validation loss that shows improvement, but a large gap remains between both curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underepresentative Training set Curves\n",
    "\n",
    "![Training Loss 1](images/under-rep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unrepresentative Validation Dataset\n",
    "\n",
    "### An unrepresentative validation dataset means that the validation dataset does not provide sufficient information to evaluate the ability of the model to generalize. This may occur if the validation dataset has too few examples as compared to the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under Representing Validation set\n",
    "\n",
    "![Training Loss 1](images/val-rep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It may also be identified by a validation loss that is lower than the training loss. In this case, it indicates that the validation dataset may be easier for the model to predict than the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training Loss 1](images/train-val-loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Learning curves are plots that show changes in learning performance over time in terms of experience.\n",
    "* Learning curves of model performance on the train and validation datasets can be used to diagnose an underfit, overfit, or well-fit model.\n",
    "* Learning curves of model performance can be used to diagnose whether the train or validation datasets are not relatively representative of the problem domain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
