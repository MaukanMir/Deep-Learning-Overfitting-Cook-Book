{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Learning curves are plots that show changes in learning performance over time in terms of experience.\n",
    "* Learning curves of model performance on the train and validation datasets can be used to diagnose an underfit, overfit, or well-fit model.\n",
    "* Learning curves of model performance can be used to diagnose whether the train or validation datasets are not relatively representative of the problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimization Learning Curves: Learning curves calculated on the metric by which the parameters of the model are being optimized, e.g. loss.\n",
    "* Performance Learning Curves: Learning curves calculated on the metric by which the model will be evaluated and selected, e.g. accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfit Learning Curves\n",
    "* Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set.\n",
    "* An underfit model can be identified from the learning curve of the training loss only. It may show a flat line or noisy values of relatively high loss, indicating that the model was unable to learn the training dataset at all.\n",
    "* A plot of learning curves shows underfitting if:\n",
    "* The training loss remains flat regardless of training.\n",
    "* The training loss continues to decrease until the end of training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss Examples\n",
    "\n",
    "![Training Loss 1](images/training-loss.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss Example\n",
    "![Training Loss 2](images/training-loss-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit Learning Curves\n",
    "\n",
    "* Overfitting refers to a model that has learned the training dataset too well, including the statistical noise or random fluctuations in the training dataset.\n",
    "* fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to a phenomenon known as overfitting the data, which essentially means they follow the errors, or noise, too closely.\n",
    "* The problem with overfitting, is that the more specialized the model becomes to training data, the less well it is able to generalize to new data, resulting in an increase in generalization error. This increase in generalization error can be measured by the performance of the model on the validation dataset.\n",
    "\n",
    "* This often occurs if the model has more capacity than is required for the problem, and, in turn, too much flexibility. It can also occur if the model is trained for too long. A plot of learning curves shows overfitting if:\n",
    "* The plot of training loss continues to decrease with experience.\n",
    "* The plot of validation loss decreases to a point and begins increasing again.\n",
    "\n",
    "* The inflection point in validation loss may be the point at which training could be halted as experience after that point shows the dynamics of overfitting. The example plot below demonstrates a case of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Curves\n",
    "\n",
    "![Training Loss 1](images/overfitt-img.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Fit Learning Curves\n",
    "\n",
    "### A good fit is the goal of the learning algorithm and exists between an overfit and underfit model. A good fit is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values.\n",
    "\n",
    "* The plot of training loss decreases to a point of stability.\n",
    "* The plot of validation loss decreases to a point of stability and has a small gap with the\n",
    "training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Fitting Curves\n",
    "\n",
    "![Training Loss 1](images/Good-fit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing Unrepresentative Datasets\n",
    "\n",
    "* An unrepresentative dataset means a dataset that may not capture the statistical characteristics relative to another dataset drawn from the same domain, such as between a train and a validation dataset. \n",
    "* This can commonly occur if the number of samples in a dataset is too small, relative to another dataset.\n",
    "\n",
    "### There are two common cases that could be observed; they are:\n",
    "\n",
    "* Training dataset is relatively unrepresentative.\n",
    "* Validation dataset is relatively unrepresentative.\n",
    "\n",
    "### An unrepresentative training dataset means that the training dataset does not provide sufficient information to learn the problem, relative to the validation dataset used to evaluate it. This may occur if the training dataset has too few examples as compared to the validation dataset. This situation can be identified by a learning curve for training loss that shows improvement and similarly a learning curve for validation loss that shows improvement, but a large gap remains between both curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underepresentative Training set Curves\n",
    "\n",
    "![Training Loss 1](images/under-rep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unrepresentative Validation Dataset\n",
    "\n",
    "### An unrepresentative validation dataset means that the validation dataset does not provide sufficient information to evaluate the ability of the model to generalize. This may occur if the validation dataset has too few examples as compared to the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under Representing Validation set\n",
    "\n",
    "![Training Loss 1](images/val-rep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It may also be identified by a validation loss that is lower than the training loss. In this case, it indicates that the validation dataset may be easier for the model to predict than the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training Loss 1](images/train-val-loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Learning curves are plots that show changes in learning performance over time in terms of experience.\n",
    "* Learning curves of model performance on the train and validation datasets can be used to diagnose an underfit, overfit, or well-fit model.\n",
    "* Learning curves of model performance can be used to diagnose whether the train or validation datasets are not relatively representative of the problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets Learn a Mapping Function\n",
    "\n",
    "* A neural network model uses the examples to learn how to map specific sets of input variables to the output variable. It must do this in such a way that this mapping works well for the training dataset, but also works well on new examples not seen by the model during training. This ability to work well on specific examples and new examples is called the ability of the model to generalize.\n",
    "* A multilayer perceptron is just a mathematical function mapping some set of input values to output values.\n",
    "* A feedforward network defines a mapping and learns the value of the parameters that result in the best function approximation.\n",
    "* As such, we can describe the broader problem that neural networks solve as function approximation. They learn to approximate an unknown underlying mapping function given a training dataset.\n",
    "*  A point on the landscape is a specific set of weights for the model, and the elevation of that point is an evaluation of the set of weights, where valleys represent good models with small values of loss. This is a common conceptualization of optimization problems and the landscape is referred to as an error surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Dimensional\n",
    "\n",
    "* The problem of navigating a high-dimensional space is that the addition of each new dimension dramatically increases the distance between points in the search space. This is often referred to as the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of the Learning Algorithm\n",
    "* Network Topology.\n",
    "* Loss Function.\n",
    "* Weight Initialization. \n",
    "* Batch Size.\n",
    "* Learning Rate.\n",
    "* Epochs.\n",
    "* Data Preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Network Topology. The number of nodes (or equivalent) in the hidden layers and the number of hidden layers in the network\n",
    "* Loss Function. The function used to measure the performance of a model with a specific set of weights on examples from the training dataset\n",
    "* Weight Initialization. The procedure by which the initial small random values are assigned to model weights at the beginning of the training process.\n",
    "* Batch Size. The number of examples used to estimate the error gradient before updating the model parameters\n",
    "* Learning Rate: The amount that each model parameter is updated per iteration of the learning algorithm\n",
    "* Epochs. The number of complete passes through the training dataset before the training process is terminated\n",
    "* Data Preparation. The schemes used to prepare the data prior to modeling in order to ensure that it is suitable for the problem and for developing a stable model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Capacity with Nodes and Layers\n",
    "\n",
    "* Neural network model capacity is controlled both by the number of nodes and the number of layers in the model.\n",
    "* A model with a single hidden layer and sufficient number of nodes has the capability of learning any mapping function, but the chosen learning algorithm may or may not be able to realize this capability.\n",
    "* Increasing the number of layers provides a short-cut to increasing the capacity of the model with fewer resources, and modern techniques allow learning algorithms to successfully train deep models.\n",
    "\n",
    "\n",
    "### A model with less capacity may not be able to sufficiently learn the training dataset. A model with more capacity can model more different functions and may be able to learn a function to sufficiently map inputs to outputs in the training dataset. Whereas a model with too much capacity may memorize the training dataset and fail to generalize or get lost or stuck in the search for a suitable mapping function. Generally, we can think of model capacity as a control over whether the model is likely to underfit or overfit a training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can control whether a model is more likely to overfit or underfit by altering its capacity.\n",
    "#### The capacity of a neural network can be controlled by two aspects of the model:\n",
    "* Number of Nodes.\n",
    "* Number of Layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Width\n",
    "* The number of nodes in a layer is referred to as the width. Developing wide networks with one layer and many nodes was relatively straightforward. In theory, a network with enough nodes in the single hidden layer can learn to approximate any mapping function, although in practice, we don’t know how many nodes are sufficient or how to train such a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth\n",
    "* The number of layers in a model is referred to as its depth. Increasing the depth increases the capacity of the model. Training deep models, e.g. those with many hidden layers, can be computationally more efficient than training a single layer network with a vast number of nodes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
