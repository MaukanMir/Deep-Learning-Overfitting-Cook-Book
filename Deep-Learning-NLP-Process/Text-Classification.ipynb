{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word Embedding: A distributed representation of words where different words that have a similar meaning (based on their usage) also have a similar representation.\n",
    "* Convolutional Model: A feature extraction model that learns to extract salient features from documents represented using a word embedding.\n",
    "* Fully Connected Model: The interpretation of extracted features in terms of a predictive output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovered via grid search and used across a suite of 7 text classification tasks, summarized as follows:\n",
    "* Transfer function: rectified linear.\n",
    "* Kernel sizes: 2, 4, 5.\n",
    "* Number of filters: 100.\n",
    "* Dropout rate: 0.5.\n",
    "* Weight regularization (L2): 3.\n",
    "* Batch Size: 50.\n",
    "* Update Rule: Adadelta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Develop an Embedding + CNN Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "# define the model\n",
    "def define_model(vocab_size, max_length):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, 100, input_length=max_length)) \n",
    "  model.add(Conv1D(filters=32, kernel_size=8, activation='relu')) \n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(10, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  # compile network\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # summarize defined model\n",
    "  model.summary()\n",
    "  plot_model(model, to_file='model.png', show_shapes=True)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/test/Documents/Software-projects/Python Projects/Deep-Learning-Projects/Deep-Learning-Overfitting-Cook-Book/data/cv000_29416.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guys', 'dies', 'girlfriend', 'continues', 'see', 'life', 'nightmares', 'whats', 'deal', 'watch', 'movie', 'sorta', 'find', 'critique', 'mindfuck', 'movie', 'teen', 'generation', 'touches', 'cool', 'idea', 'presents', 'bad', 'package', 'makes', 'review', 'even', 'harder', 'one', 'write', 'since', 'generally', 'applaud', 'films', 'attempt', 'break', 'mold', 'mess', 'head', 'lost', 'highway', 'memento', 'good', 'bad', 'ways', 'making', 'types', 'films', 'folks', 'didnt', 'snag', 'one', 'correctly', 'seem', 'taken', 'pretty', 'neat', 'concept', 'executed', 'terribly', 'problems', 'movie', 'well', 'main', 'problem', 'simply', 'jumbled', 'starts', 'normal', 'downshifts', 'fantasy', 'world', 'audience', 'member', 'idea', 'whats', 'going', 'dreams', 'characters', 'coming', 'back', 'dead', 'others', 'look', 'like', 'dead', 'strange', 'apparitions', 'disappearances', 'looooot', 'chase', 'scenes', 'tons', 'weird', 'things', 'happen', 'simply', 'explained', 'personally', 'dont', 'mind', 'trying', 'unravel', 'film', 'every', 'give', 'clue', 'get', 'kind', 'fed', 'films', 'biggest', 'problem', 'obviously', 'got', 'big', 'secret', 'hide', 'seems', 'want', 'hide', 'completely', 'final', 'five', 'minutes', 'make', 'things', 'entertaining', 'thrilling', 'even', 'engaging', 'meantime', 'really', 'sad', 'part', 'arrow', 'dig', 'flicks', 'like', 'actually', 'figured', 'halfway', 'point', 'strangeness', 'start', 'make', 'little', 'bit', 'sense', 'still', 'didnt', 'make', 'film', 'entertaining', 'guess', 'bottom', 'line', 'movies', 'like', 'always', 'make', 'sure', 'audience', 'even', 'given', 'secret', 'password', 'enter', 'world', 'understanding', 'mean', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'visions', 'minutes', 'throughout', 'movie', 'plain', 'lazy', 'okay', 'get', 'people', 'chasing', 'dont', 'know', 'really', 'need', 'see', 'giving', 'us', 'different', 'scenes', 'offering', 'insight', 'strangeness', 'going', 'movie', 'apparently', 'studio', 'took', 'film', 'away', 'director', 'chopped', 'shows', 'mightve', 'pretty', 'decent', 'teen', 'mindfuck', 'movie', 'somewhere', 'guess', 'suits', 'decided', 'turning', 'music', 'video', 'little', 'edge', 'would', 'make', 'sense', 'actors', 'pretty', 'good', 'part', 'although', 'wes', 'bentley', 'seemed', 'playing', 'exact', 'character', 'american', 'beauty', 'new', 'neighborhood', 'biggest', 'kudos', 'go', 'sagemiller', 'holds', 'throughout', 'entire', 'film', 'actually', 'feeling', 'characters', 'unraveling', 'overall', 'film', 'doesnt', 'stick', 'doesnt', 'entertain', 'confusing', 'rarely', 'excites', 'feels', 'pretty', 'redundant', 'runtime', 'despite', 'pretty', 'cool', 'ending', 'explanation', 'craziness', 'came', 'oh', 'way', 'horror', 'teen', 'slasher', 'flick', 'packaged', 'look', 'way', 'someone', 'apparently', 'assuming', 'genre', 'still', 'hot', 'kids', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'sitting', 'shelves', 'ever', 'since', 'whatever', 'skip', 'wheres', 'joblo', 'coming', 'nightmare', 'elm', 'street', 'blair', 'witch', 'crow', 'crow', 'salvation', 'lost', 'highway', 'memento', 'others', 'stir', 'echoes']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "  # open the file as read only \n",
    "  file = open(filename, 'r')\n",
    "  # read all text\n",
    "  text = file.read()\n",
    "  # close the file \n",
    "  file.close()\n",
    "  return text\n",
    "  # turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "  # split into tokens by white space\n",
    "  tokens = doc.split()\n",
    "  # prepare regex for char filtering\n",
    "  re_punc = re.compile('[%s]' % re.escape(string.punctuation)) # remove punctuation from each word\n",
    "  tokens = [re_punc.sub('', w) for w in tokens]\n",
    "  # remove remaining tokens that are not alphabetic\n",
    "  tokens = [word for word in tokens if word.isalpha()]\n",
    "  # filter out stop words\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  tokens = [w for w in tokens if not w in stop_words]\n",
    "  # filter out short tokens\n",
    "  tokens = [word for word in tokens if len(word) > 1]\n",
    "  return tokens\n",
    "# load the document\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import concatenate\n",
    "\n",
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "  # channel 1\n",
    "  inputs1 = Input(shape=(length,))\n",
    "  embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "  conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1) \n",
    "  drop1 = Dropout(0.5)(conv1)\n",
    "  pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "  flat1 = Flatten()(pool1)\n",
    "  # channel 2\n",
    "  inputs2 = Input(shape=(length,))\n",
    "  embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "  conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2) \n",
    "  drop2 = Dropout(0.5)(conv2)\n",
    "  pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "  flat2 = Flatten()(pool2)\n",
    "  # channel 3\n",
    "  inputs3 = Input(shape=(length,))\n",
    "  embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "  conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3) \n",
    "  drop3 = Dropout(0.5)(conv3)\n",
    "  pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "  flat3 = Flatten()(pool3)\n",
    "  # merge\n",
    "  merged = concatenate([flat1, flat2, flat3])\n",
    "  # interpretation\n",
    "  dense1 = Dense(10, activation='relu')(merged)\n",
    "  outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "  # compile\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # summarize\n",
    "  model.summary()\n",
    "  plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "  return model\n",
    "\n",
    "# load a clean dataset\n",
    "def load_dataset(filename):\n",
    "  return load(open(filename, 'rb'))\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(lines)\n",
    "  return tokenizer\n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "  return max([len(s.split()) for s in lines])\n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "  # integer encode\n",
    "  encoded = tokenizer.texts_to_sequences(lines)\n",
    "  # pad encoded sequences\n",
    "  padded = pad_sequences(encoded, maxlen=length, padding='post') \n",
    "  return padded\n",
    "\n",
    "# load training dataset\n",
    "trainLines, trainLabels = load_dataset('train.pkl') # create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "print('Max document length: %d' % length)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit([trainX,trainX,trainX], trainLabels, epochs=7, batch_size=16) # save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "trainLines, trainLabels = load_dataset('train.pkl') \n",
    "testLines, testLabels = load_dataset('test.pkl')\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "print('Max document length: %d' % length) \n",
    "print('Vocabulary size: %d' % vocab_size) # encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "testX = encode_text(tokenizer, testLines, length)\n",
    "print(trainX.shape, testX.shape)\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "# evaluate model on training dataset\n",
    "_, acc = model.evaluate([trainX,trainX,trainX], trainLabels, verbose=0) \n",
    "print('Train Accuracy: %.2f' % (acc*100))\n",
    "# evaluate model on test dataset dataset\n",
    "_, acc = model.evaluate([testX,testX,testX], testLabels, verbose=0) \n",
    "print('Test Accuracy: %.2f' % (acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
